{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-01T18:33:26.368849Z",
     "start_time": "2024-04-01T18:33:19.100820Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/humbertoaguilar/PycharmProjects/predictive_project/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\ndata_error = pd.read_csv('PdM_errors.csv')\\ndata_maint = pd.read_csv('PdM_maint.csv')\\ndata_failures = pd.read_csv('PdM_failures.csv')\\ndata_tele = pd.read_csv('PdM_telemetry.csv')\\ndata_machines = pd.read_csv('PdM_machines.csv')\\n\\n\\n# Fill errors data csx\\n\\ndata_error['error1'] =0\\ndata_error['error2'] =0\\ndata_error['error3'] =0\\ndata_error['error4'] =0\\ndata_error['error5'] =0\\n\\nfor error in data_error.errorID:\\n    if error == 'error1':\\n        data_error.loc[data_error.errorID == error, 'error1'] = 1\\n    elif error == 'error2':\\n        data_error.loc[data_error.errorID == error, 'error2'] = 1\\n    elif error == 'error3':\\n        data_error.loc[data_error.errorID == error, 'error3'] = 1\\n    elif error == 'error4':\\n        data_error.loc[data_error.errorID == error, 'error4'] = 1\\n    elif error == 'error5':\\n        data_error.loc[data_error.errorID == error, 'error5'] = 1\\n\\ndata_error = data_error.drop(columns=['errorID'])\\n#print(data_error)\\n\\n# Fill failures data csv\\n\\ndata_failures['component_1'] =0\\ndata_failures['component_2'] =0\\ndata_failures['component_3'] =0\\ndata_failures['component_4'] =0\\n\\nfor failure in data_failures.failure:\\n    if failure == 'comp1':\\n        data_failures.loc[data_failures.failure == failure, 'component_1'] = 1\\n    elif failure == 'comp2':\\n        data_failures.loc[data_failures.failure == failure, 'component_2'] = 1\\n    elif failure == 'comp3':\\n        data_failures.loc[data_failures.failure == failure, 'component_3'] = 1\\n    elif failure == 'comp4':\\n        data_failures.loc[data_failures.failure == failure, 'component_4'] = 1\\n\\ndata_failures = data_failures.drop(columns=['failure'])\\n#print(data_failures)\\n\\n#Fill maintenance data csv\\n\\ndata_maint['comp1_maint'] =0\\ndata_maint['comp2_maint'] =0\\ndata_maint['comp3_maint'] =0\\ndata_maint['comp4_maint'] =0\\n\\nfor maint in data_maint.comp:\\n    if maint == 'comp1':\\n        data_maint.loc[data_maint.comp == maint, 'comp1_maint'] = 1\\n    elif maint == 'comp2':\\n        data_maint.loc[data_maint.comp == maint, 'comp2_maint'] = 1\\n    elif maint == 'comp3':\\n        data_maint.loc[data_maint.comp == maint, 'comp3_maint'] = 1\\n    elif maint == 'comp4':\\n        data_maint.loc[data_maint.comp == maint, 'comp4_maint'] = 1\\n\\ndata_maint = data_maint.drop(columns=['comp'])\\n\\n\\n#print(data_maint)\\n\\n# Merge telemetry and error data csv\\n\\nerror_tele = pd.merge(data_tele, data_error, on=['datetime', 'machineID'], how='left')\\n\\n# Merge failures into error_tele\\n\\nerror_tele_fail = pd.merge(error_tele, data_failures, on=['datetime', 'machineID'], how='left')\\n#error_tele_fail\\n\\n# Merge maintenance into error_tele_fail\\n\\netf_maint = pd.merge(error_tele_fail, data_maint, on=['datetime','machineID'], how='left')\\n\\n# Merge machines into etf_maint\\n\\nfinal_dataframe = pd.merge(etf_maint, data_machines, on=['machineID'], how='left')\\n# replace NaN values with 0\\nfinal_dataframe = final_dataframe.fillna(0)\\n\\n# group by machineID and datetime so that there is only one row per machine per datetime\\nfinal_dataframe = final_dataframe.groupby(['machineID', 'datetime']).max()\\n\\n#final_dataframe\\n\\n# Save the final dataframe to a csv file\\nfinal_dataframe.to_csv('final_dataframe_with_datetime_final.csv')\\n\""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\"\"\"\n",
    "data_error = pd.read_csv('PdM_errors.csv')\n",
    "data_maint = pd.read_csv('PdM_maint.csv')\n",
    "data_failures = pd.read_csv('PdM_failures.csv')\n",
    "data_tele = pd.read_csv('PdM_telemetry.csv')\n",
    "data_machines = pd.read_csv('PdM_machines.csv')\n",
    "\n",
    "\n",
    "# Fill errors data csx\n",
    "\n",
    "data_error['error1'] =0\n",
    "data_error['error2'] =0\n",
    "data_error['error3'] =0\n",
    "data_error['error4'] =0\n",
    "data_error['error5'] =0\n",
    "\n",
    "for error in data_error.errorID:\n",
    "    if error == 'error1':\n",
    "        data_error.loc[data_error.errorID == error, 'error1'] = 1\n",
    "    elif error == 'error2':\n",
    "        data_error.loc[data_error.errorID == error, 'error2'] = 1\n",
    "    elif error == 'error3':\n",
    "        data_error.loc[data_error.errorID == error, 'error3'] = 1\n",
    "    elif error == 'error4':\n",
    "        data_error.loc[data_error.errorID == error, 'error4'] = 1\n",
    "    elif error == 'error5':\n",
    "        data_error.loc[data_error.errorID == error, 'error5'] = 1\n",
    "\n",
    "data_error = data_error.drop(columns=['errorID'])\n",
    "#print(data_error)\n",
    "\n",
    "# Fill failures data csv\n",
    "\n",
    "data_failures['component_1'] =0\n",
    "data_failures['component_2'] =0\n",
    "data_failures['component_3'] =0\n",
    "data_failures['component_4'] =0\n",
    "\n",
    "for failure in data_failures.failure:\n",
    "    if failure == 'comp1':\n",
    "        data_failures.loc[data_failures.failure == failure, 'component_1'] = 1\n",
    "    elif failure == 'comp2':\n",
    "        data_failures.loc[data_failures.failure == failure, 'component_2'] = 1\n",
    "    elif failure == 'comp3':\n",
    "        data_failures.loc[data_failures.failure == failure, 'component_3'] = 1\n",
    "    elif failure == 'comp4':\n",
    "        data_failures.loc[data_failures.failure == failure, 'component_4'] = 1\n",
    "\n",
    "data_failures = data_failures.drop(columns=['failure'])\n",
    "#print(data_failures)\n",
    "\n",
    "#Fill maintenance data csv\n",
    "\n",
    "data_maint['comp1_maint'] =0\n",
    "data_maint['comp2_maint'] =0\n",
    "data_maint['comp3_maint'] =0\n",
    "data_maint['comp4_maint'] =0\n",
    "\n",
    "for maint in data_maint.comp:\n",
    "    if maint == 'comp1':\n",
    "        data_maint.loc[data_maint.comp == maint, 'comp1_maint'] = 1\n",
    "    elif maint == 'comp2':\n",
    "        data_maint.loc[data_maint.comp == maint, 'comp2_maint'] = 1\n",
    "    elif maint == 'comp3':\n",
    "        data_maint.loc[data_maint.comp == maint, 'comp3_maint'] = 1\n",
    "    elif maint == 'comp4':\n",
    "        data_maint.loc[data_maint.comp == maint, 'comp4_maint'] = 1\n",
    "\n",
    "data_maint = data_maint.drop(columns=['comp'])\n",
    "\n",
    "\n",
    "#print(data_maint)\n",
    "\n",
    "# Merge telemetry and error data csv\n",
    "\n",
    "error_tele = pd.merge(data_tele, data_error, on=['datetime', 'machineID'], how='left')\n",
    "\n",
    "# Merge failures into error_tele\n",
    "\n",
    "error_tele_fail = pd.merge(error_tele, data_failures, on=['datetime', 'machineID'], how='left')\n",
    "#error_tele_fail\n",
    "\n",
    "# Merge maintenance into error_tele_fail\n",
    "\n",
    "etf_maint = pd.merge(error_tele_fail, data_maint, on=['datetime','machineID'], how='left')\n",
    "\n",
    "# Merge machines into etf_maint\n",
    "\n",
    "final_dataframe = pd.merge(etf_maint, data_machines, on=['machineID'], how='left')\n",
    "# replace NaN values with 0\n",
    "final_dataframe = final_dataframe.fillna(0)\n",
    "\n",
    "# group by machineID and datetime so that there is only one row per machine per datetime\n",
    "final_dataframe = final_dataframe.groupby(['machineID', 'datetime']).max()\n",
    "\n",
    "#final_dataframe\n",
    "\n",
    "# Save the final dataframe to a csv file\n",
    "final_dataframe.to_csv('final_dataframe_with_datetime_final.csv')\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T18:33:26.392549Z",
     "start_time": "2024-04-01T18:33:26.369457Z"
    }
   },
   "id": "706c966834f91e27"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n\\n\\n#Create a run_time feature by calculating the difference between the current datetime and the minimum datetime for each machineID. This can capture the cumulative run time for each machine, which might be relevant for predicting failures.\\ntemp2['run_time'] = 0\\n\\nmin_time = temp2.groupby('machineID')['datetime'].min()\\n\\nfor i in range(1,101):\\n    temp2.loc[temp2['machineID'] == i, 'run_time'] = (temp2.loc[temp2['machineID'] == i, 'datetime'] - min_time[i]).dt.total_seconds()\\n\\n\\n\\n#print each column to check if the values are correct\\n#temp= temp.drop(columns=['Unnamed: 0'])\\n\\n#temp2\\n\""
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset  \n",
    "\n",
    "data = pd.read_csv('final_dataframe_with_datetime_final.csv')\n",
    "\n",
    "# copy data to create new df to add columns for component failures\n",
    "temp = data.copy()\n",
    "\n",
    "# add columns for component failures in a day\n",
    "temp['comp1_fails_day'] = temp['component_1']\n",
    "\n",
    "# fill each of the columns with the number of component failures in a day\n",
    "day= 24\n",
    "\n",
    "for i in range(0, len(temp)):\n",
    "    if temp.iloc[i,11]== 1:\n",
    "        id = temp.iloc[i,0]\n",
    "  \n",
    "        \n",
    "        for j in range(i , max(-1,i-day) , -1):\n",
    "            \n",
    "            if id != temp.iloc[j,0]:\n",
    "                break\n",
    "            temp.at[j, 'comp1_fails_day'] = 1\n",
    "            \n",
    "temp['comp2_fails_day'] = temp['component_2']\n",
    "\n",
    "day= 24\n",
    "for i in range(0, len(temp)):\n",
    "    if temp.iloc[i,12]== 1:\n",
    "        id = temp.iloc[i,0]\n",
    "        for j in range(i , max(-1,i-day) , -1):\n",
    "            if id != temp.iloc[j,0]:\n",
    "                break\n",
    "            temp.at[j, 'comp2_fails_day'] = 1\n",
    "temp['comp3_fails_day'] = temp['component_3']\n",
    "\n",
    "day= 24\n",
    "for i in range(0, len(temp)):\n",
    "    if temp.iloc[i,13]== 1:\n",
    "        id = temp.iloc[i,0]\n",
    "        for j in range(i , max(-1,i-day) , -1):\n",
    "            if(id != temp.iloc[j,0]):\n",
    "                break\n",
    "            temp.at[j, 'comp3_fails_day'] = 1\n",
    "\n",
    "day= 24\n",
    "temp['comp4_fails_day'] = temp['component_4']\n",
    "for i in range(0, len(temp)):\n",
    "    if temp.iloc[i,14]== 1:\n",
    "        id = temp.iloc[i,0]\n",
    "        for j in range(i , max(-1,i-day) , -1):\n",
    "            if id != temp.iloc[j,0]:\n",
    "                break\n",
    "            temp.at[j, 'comp4_fails_day'] = 1\n",
    "\n",
    "temp.to_csv('temp.csv')\n",
    "\n",
    "\n",
    "\n",
    "temp = pd.read_csv('temp.csv')\n",
    "temp = temp.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "temp2 = temp.copy()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Create a run_time feature by calculating the difference between the current datetime and the minimum datetime for each machineID. This can capture the cumulative run time for each machine, which might be relevant for predicting failures.\n",
    "temp2['run_time'] = 0\n",
    "\n",
    "min_time = temp2.groupby('machineID')['datetime'].min()\n",
    "\n",
    "for i in range(1,101):\n",
    "    temp2.loc[temp2['machineID'] == i, 'run_time'] = (temp2.loc[temp2['machineID'] == i, 'datetime'] - min_time[i]).dt.total_seconds()\n",
    "\n",
    "\n",
    "\n",
    "#print each column to check if the values are correct\n",
    "#temp= temp.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "#temp2\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T18:34:19.535864Z",
     "start_time": "2024-04-01T18:33:26.380867Z"
    }
   },
   "id": "286297b4b396b9c2"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Create a list of features\n",
    "features_col = ['volt', 'rotate', 'pressure', 'vibration','age', 'error1', 'error2', 'error3', 'error4', 'comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint']\n",
    "\n",
    "target_col_day = ['comp1_fails_day', 'comp2_fails_day', 'comp3_fails_day', 'comp4_fails_day']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T18:34:19.545328Z",
     "start_time": "2024-04-01T18:34:19.535211Z"
    }
   },
   "id": "3bdcf3ccdf95bcd2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k5/8h71m_tx5pg6yssd4ssc1mbc0000gn/T/ipykernel_74557/83517286.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train = train._append(k)\n",
      "/var/folders/k5/8h71m_tx5pg6yssd4ssc1mbc0000gn/T/ipykernel_74557/83517286.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test = test._append(k)\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame(columns= temp.columns)\n",
    "test = pd.DataFrame(columns= temp.columns)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = temp2\n",
    "\n",
    "for i in range(1,101):\n",
    "  k = df[df['machineID'] == i][:6503]\n",
    "  train = train._append(k)\n",
    "  k = df[df['machineID'] == i][6503:]\n",
    "  test = test._append(k)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train[features_col] = scaler.fit_transform(train[features_col])\n",
    "test[features_col] = scaler.transform(test[features_col])\n",
    "\n",
    "\n",
    "seq_cols = features_col\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    df_zeros=pd.DataFrame(np.zeros((seq_length-1,id_df.shape[1])),columns=id_df.columns)\n",
    "    id_df=df_zeros._append(id_df,ignore_index=True)\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    lstm_array=[]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        lstm_array.append(data_array[start:stop, :])\n",
    "    return np.array(lstm_array)\n",
    "\n",
    "# function to generate labels\n",
    "def gen_label(id_df, seq_length, seq_cols,labels):\n",
    "    df_zeros=pd.DataFrame(np.zeros((seq_length-1,id_df.shape[1])),columns=id_df.columns)\n",
    "    id_df=df_zeros._append(id_df,ignore_index=True)\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    y_label=[]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        y_label.append([id_df[labels[0]][stop],id_df[labels[1]][stop],id_df[labels[2]][stop],id_df[labels[3]][stop]])\n",
    "    return np.array(y_label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T18:34:23.735900Z",
     "start_time": "2024-04-01T18:34:19.551322Z"
    }
   },
   "id": "eb23159a04082229"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650200, 48, 13)\n",
      "(650200, 4)\n",
      "(225700, 48, 13)\n",
      "(225700, 4)\n"
     ]
    }
   ],
   "source": [
    "# generate X_train\n",
    "day=48\n",
    "X_train=np.concatenate(list(list(gen_sequence(train[train['machineID']==id], day, seq_cols)) for id in train['machineID'].unique()))\n",
    "print(X_train.shape)\n",
    "# generate y_train\n",
    "y_train=np.concatenate(list(list(gen_label(train[train['machineID']==id], day, seq_cols,target_col_day) for id in train['machineID'].unique())))\n",
    "print(y_train.shape)\n",
    "# generate X_test\n",
    "X_test=np.concatenate(list(list(gen_sequence(test[test['machineID']==id], day, seq_cols)) for id in test['machineID'].unique()))\n",
    "print(X_test.shape)\n",
    "# generate y_test\n",
    "y_test=np.concatenate(list(list(gen_label(test[test['machineID']==id], day, seq_cols,target_col_day)) for id in test['machineID'].unique()))\n",
    "print(y_test.shape)\n",
    "y_train = list(np.transpose(y_train))\n",
    "y_test = list(np.transpose(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T18:34:51.596425Z",
     "start_time": "2024-04-01T18:34:23.777069Z"
    }
   },
   "id": "ace3408708a48bd1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"failure\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"failure\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m48\u001B[0m, \u001B[38;5;34m13\u001B[0m)    │          \u001B[38;5;34m0\u001B[0m │ -                 │\n│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_1 (\u001B[38;5;33mLSTM\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)        │     \u001B[38;5;34m12,800\u001B[0m │ input_layer[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_3 (\u001B[38;5;33mLSTM\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)        │     \u001B[38;5;34m12,800\u001B[0m │ input_layer[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_5 (\u001B[38;5;33mLSTM\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)        │     \u001B[38;5;34m12,800\u001B[0m │ input_layer[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_7 (\u001B[38;5;33mLSTM\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)        │     \u001B[38;5;34m12,800\u001B[0m │ input_layer[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ lstm_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ lstm_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (\u001B[38;5;33mDropout\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ lstm_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (\u001B[38;5;33mDropout\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ lstm_7[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001B[38;5;33mDense\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │         \u001B[38;5;34m51\u001B[0m │ dropout_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │         \u001B[38;5;34m51\u001B[0m │ dropout_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │         \u001B[38;5;34m51\u001B[0m │ dropout_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │         \u001B[38;5;34m51\u001B[0m │ dropout_7[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ comp1 (\u001B[38;5;33mActivation\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │          \u001B[38;5;34m0\u001B[0m │ dense[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ comp2 (\u001B[38;5;33mActivation\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │          \u001B[38;5;34m0\u001B[0m │ dense_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ comp3 (\u001B[38;5;33mActivation\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │          \u001B[38;5;34m0\u001B[0m │ dense_2[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ comp4 (\u001B[38;5;33mActivation\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │          \u001B[38;5;34m0\u001B[0m │ dense_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ comp1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ comp2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ comp3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ comp4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m51,404\u001B[0m (200.80 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,404</span> (200.80 KB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m51,404\u001B[0m (200.80 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,404</span> (200.80 KB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf\n",
    "\n",
    "class UtkMultiOutputModel():\n",
    "    def make_default_hidden_layers(self, inputs):\n",
    "        x = LSTM(units=50,return_sequences=True)(inputs)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = LSTM(units=50,return_sequences=False)(inputs)\n",
    "        x = Dropout(0.2)(x)\n",
    "        return x\n",
    "    def build_comp1_branch(self, inputs):\n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation(\"sigmoid\", name=\"comp1\")(x)\n",
    "        return x\n",
    "    def build_comp2_branch(self, inputs):\n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation(\"sigmoid\", name=\"comp2\")(x)\n",
    "        return x\n",
    "    def build_comp3_branch(self, inputs):   \n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation(\"sigmoid\", name=\"comp3\")(x)\n",
    "        return x\n",
    "\n",
    "    def build_comp4_branch(self, inputs):   \n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation(\"sigmoid\", name=\"comp4\")(x)\n",
    "        return x\n",
    "    def assemble_full_model(self, timestamp, nb_features):\n",
    "        input_shape=(timestamp, nb_features)\n",
    "        inputs = Input(shape=input_shape)\n",
    "        branch1 = self.build_comp1_branch(inputs)\n",
    "        branch2 = self.build_comp2_branch(inputs)\n",
    "        branch3 = self.build_comp3_branch(inputs)\n",
    "        branch4 = self.build_comp4_branch(inputs)\n",
    "        model = Model(inputs=inputs,\n",
    "                     outputs = [branch1,branch2,branch3,branch4],\n",
    "                     name=\"failure\")\n",
    "        return model\n",
    "nb_features =X_train.shape[2] \n",
    "#print(nb_features)\n",
    "timestamp=day\n",
    "\n",
    "    \n",
    "model = UtkMultiOutputModel().assemble_full_model(timestamp,nb_features)\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T18:34:51.736208Z",
     "start_time": "2024-04-01T18:34:51.634198Z"
    }
   },
   "id": "f6814ccc97b9fbd1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001B[1m5202/5202\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m344s\u001B[0m 66ms/step - comp1_accuracy: 0.9948 - comp2_accuracy: 0.9920 - comp3_accuracy: 0.9971 - comp4_accuracy: 0.9945 - loss: 0.0127 - val_comp1_accuracy: 0.9975 - val_comp2_accuracy: 0.9999 - val_comp3_accuracy: 0.9973 - val_comp4_accuracy: 0.9948 - val_loss: 0.0028\n",
      "Epoch 2/3\n",
      "\u001B[1m5202/5202\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m364s\u001B[0m 70ms/step - comp1_accuracy: 0.9981 - comp2_accuracy: 0.9999 - comp3_accuracy: 0.9991 - comp4_accuracy: 0.9948 - loss: 0.0021 - val_comp1_accuracy: 0.9983 - val_comp2_accuracy: 1.0000 - val_comp3_accuracy: 0.9994 - val_comp4_accuracy: 0.9949 - val_loss: 0.0019\n",
      "Epoch 3/3\n",
      "\u001B[1m 139/5202\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m5:30\u001B[0m 65ms/step - comp1_accuracy: 0.9988 - comp2_accuracy: 1.0000 - comp3_accuracy: 0.9997 - comp4_accuracy: 0.9934 - loss: 0.0017"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss={\n",
    "                  'comp1': 'binary_crossentropy',\n",
    "                  'comp2': 'binary_crossentropy',\n",
    "                  'comp3': 'binary_crossentropy', \n",
    "                  'comp4': 'binary_crossentropy'},\n",
    "              loss_weights={\n",
    "                  'comp1': 0.1,\n",
    "                  'comp2': 0.1,\n",
    "                  'comp3': 0.1, \n",
    "                  'comp4': 0.1},\n",
    "              metrics={\n",
    "                  'comp1': 'accuracy',\n",
    "                  'comp2': 'accuracy',\n",
    "                  'comp3': 'accuracy',\n",
    "                  'comp4': 'accuracy'},)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=100, validation_split=0.2)\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-01T18:34:51.735688Z"
    }
   },
   "id": "8fc39ad39f1a87d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "fb93df1d7644738a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_comp1, y_pred_comp2, y_pred_comp3, y_pred_comp4 = model.predict(X_test)\n",
    "\n",
    "y_target_comp1, y_target_comp2, y_target_comp3, y_target_comp4 = y_test\n",
    "\n",
    "\n",
    "\n",
    "y_pred_classes_comp1= np.array(list(map(lambda x: 0 if x<0.5 else 1, y_pred_comp1)))\n",
    "y_pred_classes_comp2= np.array(list(map(lambda x: 0 if x<0.5 else 1, y_pred_comp2)))\n",
    "y_pred_classes_comp3= np.array(list(map(lambda x: 0 if x<0.5 else 1, y_pred_comp3)))\n",
    "y_pred_classes_comp4= np.array(list(map(lambda x: 0 if x<0.5 else 1, y_pred_comp4)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9c4114cd8bcaca40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def conf_mat(y_true , y_pred_classes , name):\n",
    "  cm = confusion_matrix(y_true,y_pred_classes)\n",
    "  plt.figure(figsize = (12, 10))\n",
    "  cm = pd.DataFrame(cm , index = [0,1] , columns = [0,1])\n",
    "  sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
    "  plt.title('Confusion Matrix', size=10)\n",
    "  plt.xlabel('Predicted : '+name , size=6)\n",
    "  plt.ylabel('Actual : '+name ,size=6)\n",
    "  plt.show()\n",
    "  return cm\n",
    "def performance(cm):\n",
    "  #Accuracy\n",
    "  print('Accuracy : ', (cm[0][0]+cm[1][1])/(cm[0][0]+cm[1][0]+cm[0][1]+cm[1][1]))\n",
    "  #Recall\n",
    "  recall = (cm[1][1]/(cm[1][1]+cm[0][1]))\n",
    "  print('Recall : ', recall )\n",
    "  #Precision\n",
    "  precision = (cm[1][1]/(cm[1][1]+cm[1][0]))\n",
    "  print('Precision : ', precision )\n",
    "  #F1 Score\n",
    "  print('F1 score : ',2*precision*recall/(precision+recall))\n",
    "performance(conf_mat(y_target_comp1 , y_pred_classes_comp1 , 'component n°1'))\n",
    "performance(conf_mat(y_target_comp2 , y_pred_classes_comp2 , 'component n°2'))\n",
    "performance(conf_mat(y_target_comp3 , y_pred_classes_comp3 , 'component n°3'))\n",
    "performance(conf_mat(y_target_comp4 , y_pred_classes_comp4 , 'component n°4'))\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "227ca32916e56736"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_failure_probability(machine_id,comp):\n",
    "    machine_df=test[test.machineID==machine_id]\n",
    "    machine_test=gen_sequence(machine_df,day,seq_cols)\n",
    "    m_pred=model.predict(machine_test)\n",
    "    \n",
    "    failure_prob=list(m_pred[comp][-1]*100)[0]\n",
    "    return failure_prob\n",
    "     \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "fdaae6c9e3a99682"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model and weights\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.weights.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ed5edc52e0bc92e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the model and weights\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(\"model.weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d3ac56d9b560ac96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test the model\n",
    "print(compute_failure_probability(93,0))\n",
    "print(compute_failure_probability(93,1))\n",
    "print(compute_failure_probability(93,2))\n",
    "print(compute_failure_probability(93,3))\n",
    "\n",
    "print(compute_failure_probability(80,0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a7dcf866cdc5581"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8472faf972069050"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
